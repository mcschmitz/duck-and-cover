from typing import Dict

import torch
from torch import Tensor, nn

from config import GANTrainConfig
from tasks.progan import ProGANTask


class StyleGANTask(ProGANTask):
    def __init__(
        self,
        config: GANTrainConfig,
        generator: nn.Module,
        discriminator: nn.Module,
    ):
        super().__init__(
            config=config,
            generator=generator,
            discriminator=discriminator,
        )
        self.softplus = nn.Softplus()

    def train_discriminator(
        self, batch: Dict[str, torch.Tensor], generated_images: Tensor
    ):
        """
        Runs a single gradient update on a batch of data.

        Args:
            batch: Real input images used for training
            generated_images: Images generated by the generator
        """
        _, discriminator_optimizer = self.optimizers()
        discriminator_optimizer.zero_grad()

        real_imgs = batch["images"]
        real_pred = self.discriminator(
            images=real_imgs,
            year=batch.get("year"),
            block=self.block,
            alpha=self.alpha,
        )
        fake_pred = self.discriminator(
            images=generated_images,
            year=batch.get("year"),
            block=self.block,
            alpha=self.alpha,
        )
        loss = torch.mean(self.softplus(fake_pred)) + torch.mean(
            self.softplus(-real_pred)
        )
        real_imgs = torch.autograd.Variable(real_imgs, requires_grad=True)
        real_logit = self.discriminator(
            images=real_imgs,
            year=batch.get("year"),
            block=self.block,
            alpha=self.alpha,
        )
        real_grads = torch.autograd.grad(
            outputs=real_logit,
            inputs=real_imgs,
            grad_outputs=torch.ones(real_logit.size()).to(self.device),
            create_graph=True,
            retain_graph=True,
        )[0].view(real_imgs.size(0), -1)
        r1_penalty = torch.sum(torch.mul(real_grads, real_grads))
        loss += r1_penalty
        self.manual_backward(loss)
        discriminator_optimizer.step()

        real = torch.zeros((self.config.batch_size, 1)).to(self.device)
        fake = torch.ones((self.config.batch_size, 1)).to(self.device)
        fake_pred = self.sigmoid(fake_pred)
        real_pred = self.sigmoid(real_pred)
        correct_fake = torch.sum(torch.round(fake_pred) == fake).cpu().numpy()
        correct_real = torch.sum(torch.round(real_pred) == real).cpu().numpy()
        discriminator_acc = (correct_real + correct_fake) / (
            self.config.batch_size * 2
        )
        self.log("train/discriminator_loss", loss)
        self.log("train/discriminator_accuracy", discriminator_acc)

    def train_generator(
        self, batch: Dict[str, torch.Tensor], noise: torch.Tensor
    ):
        """
        Trains the generator on a single batch of data and returns the
        Generator loss.

        Args:
            batch: Input batch containing images and year
            noise: Randomly generated noise
        """
        generator_optimizer, _ = self.optimizers()
        generator_optimizer.zero_grad()
        noise = noise.to(self.device)
        generated_images = self.generator(
            x=noise, year=batch.get("year"), block=self.block, alpha=self.alpha
        )
        g_loss = torch.mean(
            self.softplus(
                -self.discriminator(
                    images=generated_images,
                    year=batch.get("year"),
                    block=self.block,
                    alpha=self.alpha,
                )
            )
        )
        self.manual_backward(g_loss)
        generator_optimizer.step()
        self.log("train/generator_loss", g_loss)
        self.update_ema_generator()
